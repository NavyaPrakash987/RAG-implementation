import os
import fitz  # PyMuPDF
import docx
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders import PyMuPDFLoader, UnstructuredWordDocumentLoader, TextLoader

# Step 1: Load Documents
def load_documents(folder_path):
    docs = []
    for file in os.listdir(folder_path):
        file_path = os.path.join(folder_path, file)
        if file.endswith(".pdf"):
            loader = PyMuPDFLoader(file_path)
        elif file.endswith(".docx"):
            loader = UnstructuredWordDocumentLoader(file_path)
        elif file.endswith(".txt"):
            loader = TextLoader(file_path)
        else:
            continue
        docs.extend(loader.load())
    return docs

# Step 2: Split Text into Chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)

def split_documents(docs):
    return text_splitter.split_documents(docs)

# Step 3: Embed the Text
embeddings = OpenAIEmbeddings()

def create_vector_store(chunks):
    return FAISS.from_documents(chunks, embeddings)

# Step 4: Store in Vector DB
def store_vectors(vector_store, path="faiss_index"):
    vector_store.save_local(path)

def load_vectors(path="faiss_index"):
    return FAISS.load_local(path, embeddings)

# Step 5: Querying the Vector Store
def retrieve_documents(query, vector_store, k=3):
    retriever = vector_store.as_retriever(search_kwargs={"k": k})
    return retriever.get_relevant_documents(query)

# Step 6: Retrieve Context for Generation
def generate_answer(query, vector_store, model="text-davinci-003"):
    retriever = vector_store.as_retriever()
    qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(model_name=model), chain_type="stuff", retriever=retriever)
    return qa_chain.run(query)

# Example usage
folder_path = "./documents"  # Change this to your folder path
documents = load_documents(folder_path)
chunks = split_documents(documents)
vector_store = create_vector_store(chunks)
store_vectors(vector_store)

# Load and Query
db = load_vectors()
query = "What is Retrieval-Augmented Generation?"
response = generate_answer(query, db)
print(response)

